# Configuration for Cog
# https://cog.run/yaml

build:
  gpu: true
  cuda: "12.2"
  python_version: "3.11"
  system_packages:
    - "cmake"
    - "build-essential"
  run:
    # Force llama-cpp-python to build everything from source (GLIBC compat)
    # Pin numpy<2.4 for numba compatibility
    - "pip install 'numpy<2.4'"
    - "pip uninstall -y llama-cpp-python || true"
    - "LLAMA_CPP_LIB= CMAKE_ARGS='-DGGML_NATIVE=OFF' pip install llama-cpp-python==0.3.9 --no-binary llama-cpp-python --no-cache-dir"
  python_packages:
    - "torch==2.5.1"
    - "torchaudio==2.5.1"
    - "transformers>=4.45.0"
    - "outetts>=0.3.0"
    - "huggingface_hub>=0.25.0"

predict: "predict.py:Predictor"
